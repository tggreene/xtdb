= Azure Auctionmark Benchmark

Contained within this folder are a number of things for setting up & running a cluster of containerized auctionmark running worker tasks on Azure Platform.

Within this README is the following:

* Instructions for setting up the docker image.
* How to setup the infra necessary on Azure.
* How to push the docker image to Azure.
* How to deploy the benchmark containers.

== Requirements

For local development, you will need the following:

* To run the various scripts and commands within this folder, you will need the `az` command line tool, and to be authenticated with it.
* `docker` available on your machine.
* `terraform` available on your machine.
* `kubectl` available on your machine.

=== Authenticating with Azure

Firstly, you want to login to Azure using the command line - this can be done by running:
```bash
az login --scope https://management.azure.com//.default
```

After this, you want to ensure that you are using the correct subscription - in our case, that will be "XTDB long-run reliability":
```bash
az account set --subscription "XTDB long-run reliability"
```

== Creating the Docker Image

Included within this directory is a subdirectory of scripts. To build an image for azure, you can run the following script:
```bash
./scripts/build-azure-bench.sh
```

This will:

* Creates a Shadow JAR of the `cloud-benchmark:azure` project.
** This contains the compiled `cloud-benchmark` project, which has a main file that runs auctionmark & configures it with provided node config, and also any of the dependencies we require to run the Azure module.
* Creates a docker image, copying in the Shadow JAR and the `azure-config.yaml` file with the node config.
** If you want to set any JVM opts - do it by editing the Dockerfile's `ENTRYPOINT`.

It will build the local image as `xtdb-azure-bench:latest`.

== Setting up the Azure Infra

All of the config for setting up the infrastructure on Azure is handled by Terraform - and the files are within the `terraform` directory. 

* The `main.tf` file contains the main configuration for the Azure resources.
* Can update the names of a few things within `terraform.tfvars` - though this is not expected to change so much from the default values. The only thing that may be expected to change is the `kubernetes_vm_size` if you wish to run more than one pod/multiple nodes.

Prior to deploying any of the infra using terraform, you will need to initialize the terraform directory. From the `terraform` directory:
```bash
terraform init
```

To ensure that the terraform config is properly formatted, you can run:
```bash
terraform fmt
```

To then validate the terraform config, you can run:
```bash
terraform validate
```

To see what will will/change when running terraform, you can run:
```bash
terraform plan
```

To actually deploy the infra, you can run:
```bash
terraform apply
```

This will setup all of the azure resources we use and a base 

== Pushing the Docker Image to Azure

To push the docker image to Azure, you will need to use the Azure Container Registry.

To login to the Azure Container Registry, you can run:
```bash
az acr login --name cloudbenchmarkregistry
```

You can then tag the image you built earlier:
```bash
docker tag xtdb-azure-bench cloudbenchmarkregistry.azurecr.io/xtdb-azure-bench
```

Finally, to push the image to the Azure Container Registry, you can run:
```bash
docker push cloudbenchmarkregistry.azurecr.io/xtdb-azure-bench
```

== Deploying the Benchmark Containers


Before trying to deploy the benchmark containers, you will need to ensure that you have the necessary permissions to deploy to the Kubernetes cluster. You can do this by running the following command:
```
az aks get-credentials --resource-group cloud-benchmark-resources --name cloud-benchmark-cluster
```

This will configure your `kubectl` to use the credentials for the Kubernetes cluster. 

Before deploying anything, must ensure all of the necessary pieces of config are set within the ConfigMap/match the values/names of the terraform architecture, run the following in the terraform directory:
```
terraform outputs
```

Ensure that the contents of the files under `kubernetes` match the values of the terraform outputs.

The config for the deployments/jobs live under `kubernetes` - within these, you can set/configure any of the necessary parameters for running the image on the cluster. When ready to run a single node auctionmark job, run the following command from the base of this directory:
```
kubectl apply -f kubernetes/single-node-auctionmark.yaml
```

You can see the status of job creation using the following:
```
kubectl get jobs --namespace cloud-benchmark
```

Trail the logs of the single node run by running:
```
kubectl logs job.batch/xtdb-single-node-auctionmark --namespace cloud-benchmark -f
```

== Clear up between runs

If you want to totally clear up data between runs, you'll want to do the following:

* Clear up the job/pods
* Empty the Azure Storage Blobs Container
* Delete the Persistent Storage volume used by the TxLog
* Delete the Persistent Storage volume containing the Local Disk Caches

.Clear up the Workload

To clear up the workload, you can do the following
```bash
kubectl delete jobs xtdb-single-node-auctionmark --namespace cloud-benchmark
```

.Command to empty the Azure Storage Blob Container:
```bash
./clear-azure-storage.sh
```

.Deleting Persistent Storage Volumes:
You can remove the Persistent Storage volumes within the google cloud UI, but will need to be careful to ensure they are both removed from GKE and deleted within Compute Engine's storage as well. You will need to ensure any pods are closed/deleted first, and then to delete them, you can do the following:
```bash
kubectl delete pvc xtdb-pvc-log --namespace cloud-benchmark
kubectl delete pvc xtdb-pvc-local-caches --namespace cloud-benchmark
``` 

NOTE: You do not necessarily _need_ to delete all of the above between runs - you can also change the kubernetes config map to use slightly different directory names (ie, changing bucket prefix, new local-disk-cache directory, etc) to avoid conflicts between runs. 

== Monitoring

To see the benchmark metrics you need to supply an application insights instrumentation key via an environment variable in the `single-node-auctionmark.yaml` or `multi-node-auctionmark.yaml` file as below:

```yaml
apiVersion: "v1"
kind: "ConfigMap"
metadata:
  ...
data:
  XTDB_AZURE_INSTRUMENTATION_KEY: "<instrumentation-key>"
  ...
```

You can retrieve the instrumentation key from terraform state as follows:

```bash
terraform output -raw insights_instrumentation_key
```

After a while you should be able to see the metrics in the Azure portal under the Application Insights resource, navigating to Monitoring > Metrics and looking under Metric Namespace > Custom.

---
title: Setting up a cluster on Azure
---

This guide will walk you through the process of configuring and running an XTDB Cluster on Azure. This setup includes:

* Using **Azure Blob Storage** as the remote storage implementation.
* Utilizing **Apache Kafka** as the shared message log implementation.
* Exposing the cluster to the internet via a Postgres wire-compatible server and HTTP.
 
The required Azure infrastructure is provisioned using **Terraform**, and the XTDB cluster and it's resources are deployed on link:https://azure.microsoft.com/en-us/products/kubernetes-service[**Azure Managed Kubernetes Service (AKS)**^] using **Helm**, 

Although we provide numerous parameters to configure the templates, you are encouraged to edit them, use them as a foundation for more advanced use cases, and reuse existing infrastructure when suitable. 
These templates serve as a simple starting point for running XTDB on Azure and Kubernetes, and should be adapted to meet your specific needs, especially in production environments.

This guide assumes that you are using the default templates.

== Requirements 

Before starting, ensure you have the following installed:

* The **Azure CLI** - See the link:https://learn.microsoft.com/en-us/cli/azure/[**Installation Instructions**^].
* **Terraform** - See the link:https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli[**Installation Instructions**^].
* **kubectl** - The Kubernetes CLI, used to interact with the AKS cluster. See the link:https://kubernetes.io/docs/tasks/tools/install-kubectl/[**Installation Instructions**^].
* **Helm** - The Kubernetes package manager, used to deploy numerous components to AKS. See the link:https://helm.sh/docs/intro/install/[**Installation Instructions**^].

=== Authenticating the Azure CLI

Within Azure, ensure that you have an existing Subscription, and that you are authenticated with the Azure CLI.

Ensure that your existing Subscription has the necessary resource providers - see link:https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/resource-providers-and-types[**this article**^] for more information. This guide requires the following providers:

* `Microsoft.ContainerService` - for the AKS resources.
* `Microsoft.ManagedIdentity` - for the user assigned managed identity resources.
* `Microsoft.Storage` - for the storage account resources. 

To login to Azure using the command line, run the following:

```bash
az login --scope https://management.azure.com//.default
```

To explicitly check that CLI commands run against the correct subscription, run:

```bash
az account set --subscription "Subscription Name"
```

This allows you to perform necessary operations on Azure via Terraform using the User Principal on the Azure CLI.

NOTE: There are other ways to authenticate Terraform with Azure besides using the User Principal available via the Azure CLI. 
For other authentication scenarios, see the link:https://developer.hashicorp.com/terraform/language/settings/backends/azurerm[**azurerm backend authentication**^] docs.

== Getting started with Terraform

The following assumes that you are authenticated on the Azure CLI, have Terraform installed on your machine, and are located a directory that you wish to use as the root of the Terraform configuration.

First, make the following `terraform init` call:
```
terraform init -from-module github.com/xtdb/xtdb.git//azure/terraform
```  

This will download the Terraform files from the XTDB repository, and initialize the working directory.

NOTE: For the sake of this guide, we store Terraform state locally. 
However, to persist the state onto Azure, you will need to configure a remote backend using Azure Blob Storage. 
This allows you to share the state file across teams, maintain versioning, and ensure consistency during deployments. 
For more info, see the link:https://developer.hashicorp.com/terraform/language/backend/azurerm[**Terraform azurem backend**^] documentation.

== What is being deployed on Azure?

The sample Terraform directory sets up a few distinct parts of the infrastructure required by XTDB. 
If using the default configuration, the following will be created:

* **XTDB Resource Group and User Assigned Managed Identity**  
* **Azure Storage Account**  (with a container for object storage)
** Configured with associated resources using the link:https://registry.terraform.io/modules/Azure/avm-res-storage-storageaccount/azurerm/latest[**Azure/avm-res-storage-storageaccount**^] Terraform module.
** Adds required permissions to the User Assigned Managed Identity.
* **AKS Cluster**  
** Configured with associated resources using the link:https://registry.terraform.io/modules/Azure/aks/azurerm/latest[**Azure/aks**^] Terraform module.

NOTE: The above infrastructure is designed for creating a simple starting point for running XTDB on Azure & Kubernetes. 
The VM sizes and resource tiers can & should be adjusted to suit your specific requirements and cost constraints, and the templates should be configured with any desired changes to security or networking configuration.

== Deploying the Azure Infrastructure

Before creating the Terraform resources, review and update the `terraform.tfvars` file to ensure the parameters are correctly set for your environment:

* You are **required** to set a unique and valid `storage_account_name` for your environment.
* You may also wish to change resource tiers, the location of the resource group, or the VM sizes used by the AKS cluster.
** The VM sizes used within the examples may not always be available in your subscription - if this is the case, see alternative/equivalent VM sizes that you can use within the link:https://docs.microsoft.com/en-us/azure/virtual-machines/sizes[**Azure VM Sizes**^] document.  
** Ensure that the quota for the VM size and region is set appropriately in `Subscription > Settings > Usage + Quotas`.

To get a full list of the resources that will be deployed by the templates, run:
```bash
terraform plan
```

Finally, to create the resources, run:
```bash
terraform apply
```

This will create all the resources within the Azure subscription and save the state of the resources within the storage account created earlier. 

[#terraform-outputs]
=== Fetching the Terraform Outputs

The Terraform templates will generate several outputs required for setting up the XTDB nodes on the AKS cluster.

To retrieve these outputs, execute the following command:
```bash
terraform output
```

This will return the following outputs:

* `storage_account_container`
* `storage_account_name`
* `user_managed_identity_client_id`

== Deploying on Kubernetes

With the infrastructure created on Azure, you can now deploy the XTDB nodes and a simple Kafka instance on the AKS cluster.

Prior to deploying the Kubernetes resources, ensure that the kubectl CLI is installed and configured to deploy and connect to the AKS cluster. Run the following command:

```bash
az aks get-credentials --resource-group xtdb-resource-group --name xtdb-aks-cluster
```

Now that `kubectl` is authenticated with the AKS cluster, you can set up the namespace for the XTDB deployment:

```bash
kubectl create namespace xtdb-deployment
```

The AKS cluster is now ready for deployment,

'''

=== Deploying an example Kafka 

To deploy a basic set of Kafka resources within AKS, you can make use of the `bitnami/kafka` Helm chart. Run the following command:

```bash
helm install kafka oci://registry-1.docker.io/bitnamicharts/kafka \
  --version 31.3.1 \
  --namespace xtdb-deployment \
  --set listeners.client.protocol=PLAINTEXT \
  --set listeners.controller.protocol=PLAINTEXT \
  --set controller.resourcesPreset=medium \
  --set controller.nodeSelector.node_pool=xtdbpool
```

This command will create:

* A simple, **unauthenticated** Kafka deployment on the AKS cluster, which XTDB will use as its message log, along with its dependent infrastructure and persistent storage.
* A Kubernetes service to expose the Kafka instance to the XTDB cluster.

==== Considerations of the Kafka Deployment

The Kafka instance set up above is for **demonstration purposes only** and is **not recommended for production use**. 
This example lacks authentication for the Kafka cluster and allows XTDB to manage Kafka topic creation and configuration itself.

For production environments, consider the following:

* Use a more robust Kafka deployment.
* Pre-create the required Kafka topics.
* Configure XTDB appropriately to interact with the production Kafka setup.

Additional resources:

* For further configuration options for the Helm chart, refer to the link:https://artifacthub.io/packages/helm/bitnami/kafka[**Bitnami Kafka Chart Documentation**^].
* For detailed configuration guidance when using Kafka with XTDB, see the link:https://docs.xtdb.com/ops/config/log/kafka.html#_setup[**XTDB Kafka Setup Documentation**^].

=== Verifying the Kafka Deployment

After deployment, verify that the Kafka instance is running properly by checking its status and logs.

To check the status of the Kafka deployment, run the following command:
```bash
kubectl get pods --namespace xtdb-deployment
```

To view the logs of the Kafka deployment, use the command:
```bash
kubectl logs -f statefulset/kafka-controller --namespace xtdb-deployment
```

By verifying the status and reviewing the logs, you can ensure the Kafka instance is correctly deployed and ready for use by XTDB.

'''

=== Setting up the XTDB Workload Identity

In order for the XTDB nodes to access an Azure storage account securely, a Kubernetes Service Account (KSA) must be set up and linked to a User Assigned Managed Identity using link:https://learn.microsoft.com/en-us/entra/workload-id/workload-identity-federation[**Workload Identity Federation**^].

To set up the Kubernetes Service Account, run the following command:

```bash
kubectl create serviceaccount xtdb-service-account --namespace xtdb-deployment
```

Fetch the name of the User Assigned Managed Identity (`user_assigned_managed_identity_name`) and the OIDC issuer URL of the AKS cluster (`oidc_issuer_url`) from the Terraform outputs.

To create the federated identity run the `az` CLI command:

```bash
az identity federated-credential create \
  --name "xtdb-federated-identity" \
  --resource-group "xtdb-resource-group" \
  --subject "system:serviceaccount:xtdb-deployment:xtdb-service-account" \
  --audience "api://AzureADTokenExchange" \
  --identity-name "<user_assigned_managed_identity_name>" \
  --issuer "<oidc_issuer_url>" 
```

The subject name must include the namespace and Kubernetes ServiceAccount name. 

Fetch the client ID of the User Assigned Managed Identity (`user_assigned_managed_identity_client_id`), and use it to annotate the Kubernetes Service Account to establish the link between the KSA and the User Assigned Managed Identity:

```bash
kubectl annotate serviceaccount xtdb-service-account \
  --namespace xtdb-deployment \
  azure.workload.identity/client-id="<user_assigned_managed_identity_client_id>"
```

With the XTDB service account set up, we can now deploy the XTDB cluster to the GKE cluster.

'''

=== Deploying the XTDB cluster

In order to deploy the XTDB cluster and it's constituent parts into the AKS cluster, we provide an `xtdb-azure` Helm chart/directory.

This can be found on the link:https://github.com/xtdb/xtdb/pkgs/container/helm-xtdb-azure[**XTDB Github Container Registry**^], and can be used directly with `helm` commands.

With the values from the link:#terraform-outputs[Terraform outputs], you can now deploy the XTDB cluster. 
Run the following command, substituting the values as appropriate: 

```bash
helm install xtdb-azure oci://ghcr.io/xtdb/helm-xtdb-azure \
  --version 2.0.0-snapshot \
  --namespace xtdb-deployment \
  --set xtdbConfig.serviceAccount="xtdb-service-account" \
  --set xtdbConfig.storageContainerName=<storage_account_container> \
  --set xtdbConfig.storageAccountName=<storage_account_name> \
  --set xtdbConfig.userManagedIdentityClientId=<user_managed_identity_client_id> 
```

The following are created by the templates:

* A `ConfigMap` containing the XTDB YAML configuration.
* A `StatefulSet` containing the XTDB nodes.
* A `LoadBalancer` Kubernetes service to expose the XTDB cluster to the internet.

To check the status of the XTDB statefulset, run:
```bash
kubectl get statefulset --namespace xtdb-deployment
```

To view the logs of each individual StatefulSet member, run:
```bash
kubectl logs -f xtdb-statefulset-n --namespace xtdb-deployment
```

==== Customizing the XTDB Deployment

The above deployment uses the `xtdb-azure` chart defaults, individually setting the terraform outputs as `xtdbConfig` settings using the command line. 

For more information on the available configuration options and fetching the charts locally for customization, see the link:/ops/azure#helm[`xtdb-azure` Helm documentation]

'''

=== Accessing the XTDB Cluster

NOTE: As it will take some time for the XTDB nodes to be marked as ready (as they need to pass their initial startup checks) it may take a few minutes for the XTDB cluster to be accessible.

NOTE: The xtdb service is only available via ClusterIP by default so as to not expose the service publicly

Once the XTDB cluster is up and running, you can access it via the ClusterIP service that was created.

To port forward the service locally
```bash
kubectl port-forward service/xtdb-service --namespace xtdb-deployment 8080:8080
```

You can do the same for the following components:

* Postgres Wire Server (on port `5432`)
* Healthz Server (on port `8080`)
* HTTP Server (on port `3000`).

To check the status of the XTDB cluster using the forwarded port, run:

```bash
curl http://localhost:8080/healthz/alive

# alternatively `/healthz/started`, `/healthz/ready`
```

If the above command succeeds, you now have a running XTDB cluster.
